import os
from dotenv import load_dotenv
load_dotenv()

from langchain_community.document_loaders import TextLoader, DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI

### LLM and Embeddings ###
llm = ChatGoogleGenerativeAI(model="gemini-1.0-pro")
embeddings = GoogleGenerativeAIEmbeddings(model="models/embedding-001")

### Step 1: Load Text Files ###
loader = DirectoryLoader('../data', glob="*.txt", loader_cls=TextLoader)
docs = loader.load()

### Step 2: Split into Chunks ###
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=50,
    chunk_overlap=10
)
split_docs = text_splitter.split_documents(docs)

### Step 3: Store Embeddings in Local Vector DB (Chroma) ###
vector_db = Chroma.from_documents(split_docs, embeddings, persist_directory="./chroma_db")
retriever = vector_db.as_retriever(search_kwargs={"k": 4})

### Step 4: RAG Prompt Template ###
template = """Answer the question based only on the following context:
{context}

Question: {question}
"""
prompt = PromptTemplate.from_template(template)

### Step 5: Retrieval + Generation Chain ###
retrieval_chain = (
    RunnableParallel({
        "context": retriever,
        "question": RunnablePassthrough()
    })
    | prompt
    | llm
    | StrOutputParser()
)

### Step 6: Run Query ###
question = "what is llama3? can you highlight 3 important points?"
print(retrieval_chain.invoke(question))
